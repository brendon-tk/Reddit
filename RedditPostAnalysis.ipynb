{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "128laCALNihB"
   },
   "outputs": [
    {
     "ename": "PySparkRuntimeError",
     "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPySparkRuntimeError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[1;32m----> 2\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReddit Post Analysis\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mgetOrCreate()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\session.py:497\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    495\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[0;32m    496\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39mgetOrCreate(sparkConf)\n\u001b[0;32m    498\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[0;32m    500\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\context.py:515\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 515\u001b[0m         SparkContext(conf\u001b[38;5;241m=\u001b[39mconf \u001b[38;5;129;01mor\u001b[39;00m SparkConf())\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    199\u001b[0m     )\n\u001b[1;32m--> 201\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[0;32m    204\u001b[0m         master,\n\u001b[0;32m    205\u001b[0m         appName,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[0;32m    216\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\context.py:436\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[1;32m--> 436\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m launch_gateway(conf)\n\u001b[0;32m    437\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\java_gateway.py:107\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    104\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[1;32m--> 107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[0;32m    108\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJAVA_GATEWAY_EXITED\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    109\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[0;32m    110\u001b[0m     )\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[0;32m    113\u001b[0m     gateway_port \u001b[38;5;241m=\u001b[39m read_int(info)\n",
      "\u001b[1;31mPySparkRuntimeError\u001b[0m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Reddit Post Analysis').getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tQfKocZIN9at",
    "outputId": "e8e1f2c6-2d9c-4c5e-84c5-99b6463db5a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+-----------------+--------------------+--------------------+--------------------+---------------+---------+---------+--------------------+---------------------+--------------+-------------+\n",
      "|     id|           timestamp|       timestamp_est|        subreddit|               title|         description|            keywords|score_immediate|score_3hr|score_8hr|         top_comment|top_comment_sentiment|post_sentiment|     Category|\n",
      "+-------+--------------------+--------------------+-----------------+--------------------+--------------------+--------------------+---------------+---------+---------+--------------------+---------------------+--------------+-------------+\n",
      "|1jxwrkv|2025-04-13 01:41:...|2025-04-12 21:41:...|NoStupidQuestions|Do fat/obese/over...|or does the same ...|['fat', 'obese', ...|              1|        0|        0|I grew up very sk...|             Positive|       Neutral|    Education|\n",
      "|1jxwrk8|2025-04-13 01:41:...|2025-04-12 21:41:...|           Advice|My friend is a ba...|Right after getti...|['friend', 'bad',...|              1|        2|        2|He's just stupid....|             Negative|      Negative|Relationships|\n",
      "|1jxwrk7|2025-04-13 01:41:...|2025-04-12 21:41:...|         buildapc|Where does the 60...|Iâ€™m in the middle...|                  []|              1|        1|        1|You need to use t...|              Neutral|       Neutral|         Tech|\n",
      "|1jxwrex|2025-04-13 01:41:...|2025-04-12 21:41:...|NoStupidQuestions|Are orange cats a...|I feel like I'm b...|['orange', 'cats'...|              1|        0|        0|Mine's pretty dum...|             Positive|       Neutral|    Education|\n",
      "|1jxwred|2025-04-13 01:41:...|2025-04-12 21:41:...|           Advice|Where can I solo ...|Hi, I'm trying to...|['solo', 'travel'...|              1|        0|        0|                NULL|                 NULL|       Neutral|Relationships|\n",
      "+-------+--------------------+--------------------+-----------------+--------------------+--------------------+--------------------+---------------+---------+---------+--------------------+---------------------+--------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .option(\"multiLine\", True) \\\n",
    "    .option(\"quote\", '\"') \\\n",
    "    .option(\"escape\", '\"') \\\n",
    "    .option(\"mode\", \"DROPMALFORMED\") \\\n",
    "    .csv('reddit_posts.csv')\n",
    "\n",
    "df_clean = df.filter(\"id IS NOT NULL AND subreddit IS NOT NULL AND title IS NOT NULL and id LIKE '1%'\")\n",
    "df_clean.createOrReplaceTempView(\"posts\")\n",
    "\n",
    "df_clean.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ojTt0BCOpiD",
    "outputId": "77a1f5b1-8e3c-4bd6-c106-ddbbba600c65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+\n",
      "|     category|count(1)|\n",
      "+-------------+--------+\n",
      "|    Education|    1429|\n",
      "|Entertainment|     613|\n",
      "|      Finance|     729|\n",
      "|Relationships|    3229|\n",
      "|         Tech|    1009|\n",
      "|    Wellbeing|    1274|\n",
      "|        Humor|     639|\n",
      "|         News|     655|\n",
      "+-------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select category, count(*) from posts group by 1 ;\n",
    "\n",
    "\"\"\").show() # people post FAR more on reddit about their relationships than anything else. people love to yap. interpersonals matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iQR-loETO4y8",
    "outputId": "cd1f8ed8-8fcd-4cb1-83fb-bb9fa5c6d78a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|    9577|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select count(*) from posts;\n",
    "\n",
    "\"\"\").show() # 30 rows were corrupt so after dropping (assume MCAR) we're left with that number of posts and rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fYumzOxLO5QH",
    "outputId": "58d4c96e-0949-45d2-f9b9-39e8f1983075"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+------------------+\n",
      "|     category|    avg(score_3hr)|    avg(score_8hr)|\n",
      "+-------------+------------------+------------------+\n",
      "|        Humor| 378.2396551724138|          1093.875|\n",
      "|         News| 339.9567387687188|  676.603139013453|\n",
      "|      Finance| 90.58233532934132|126.70742358078603|\n",
      "|    Education| 80.42262372348782| 190.8480603448276|\n",
      "|Entertainment|36.012727272727275| 87.48556430446195|\n",
      "|         Tech| 28.76816239316239|39.439169139465875|\n",
      "|    Wellbeing|25.212094653812446| 94.61864406779661|\n",
      "|Relationships| 7.774331550802139|12.768020304568529|\n",
      "+-------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select category, avg(score_3hr), avg(score_8hr) from posts group by 1 order by 2 desc ;\n",
    "\n",
    "\"\"\").show() # relationships (despite being posted on a ton) threads get like no likes on posts. people use reddit primarily to laugh (humor) and interestingly, for news. this is not traditional news outlet but people use it for their news\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vdU5unGGO54V",
    "outputId": "4144a932-5518-499f-bce9-4384b1c86d86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+------------------+------------------+\n",
      "|     category|avg_score_positive| avg_score_neutral|avg_score_negative|\n",
      "+-------------+------------------+------------------+------------------+\n",
      "|    Education| 399.6496815286624|32.840966921119595|41.878787878787875|\n",
      "|Entertainment| 37.65983606557377|24.940425531914894| 67.95714285714286|\n",
      "|      Finance| 99.13793103448276| 61.57707509881423| 111.7378277153558|\n",
      "|        Humor| 354.6456692913386| 329.1105527638191|453.26237623762376|\n",
      "|         News|142.95744680851064|100.79389312977099|  472.531914893617|\n",
      "|Relationships| 4.685258964143427|11.899103139013453| 7.269449715370019|\n",
      "|         Tech| 4.526153846153846|19.957013574660632|  98.4319526627219|\n",
      "|    Wellbeing|            14.375|              10.6|31.729333333333333|\n",
      "+-------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT category,\n",
    "       AVG(CASE WHEN post_sentiment = 'Positive' THEN score_3hr END) AS avg_score_positive,\n",
    "       AVG(CASE WHEN post_sentiment = 'Neutral' THEN score_3hr END) AS avg_score_neutral,\n",
    "       AVG(CASE WHEN post_sentiment = 'Negative' THEN score_3hr END) AS avg_score_negative\n",
    "FROM posts\n",
    "GROUP BY category\n",
    "ORDER BY category;\n",
    "\"\"\").show()\n",
    "# in 6/8 categories, the negative posts do far better than the neutral or positive posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_QMbFCyKyrrF",
    "outputId": "bb3b8241-90f6-4a4b-afb0-81cc397e72e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+-----------+--------------+\n",
      "|title                                                                                                                                                                                                                                                                                                     |subreddit    |description|post_sentiment|\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+-----------+--------------+\n",
      "|Thereâ€™s always one ðŸ˜‚                                                                                                                                                                                                                                                                                     |funny        |NULL       |Neutral       |\n",
      "|TIL Charles Dance (Tywin Lannister) always ended scenes with co-star Peter Dinklage (Tyrion Lannister) by apologizing for his character's awful comments and behavior. Dance said Dinklage is \"the most adorable man. After all those scenes, I apologize to him\" because \"I have to treat him like shit.\"|todayilearned|NULL       |Positive      |\n",
      "|TIL Reggie from Nintendo had to fight to get Wii Sports as a pack-in, free.  And Miyamoto was not happy.                                                                                                                                                                                                  |todayilearned|NULL       |Positive      |\n",
      "|TIL Schizophrenics who are born deaf will hallucinate disembodied hands signing to them, rather than hearing voices.                                                                                                                                                                                      |todayilearned|NULL       |Neutral       |\n",
      "|Adults never outgrow bullying behavior                                                                                                                                                                                                                                                                    |Adulting     |NULL       |Negative      |\n",
      "|Never interrupt your enemy while he is making a mistake.                                                                                                                                                                                                                                                  |facepalm     |NULL       |Positive      |\n",
      "|why are they doing this?                                                                                                                                                                                                                                                                                  |memes        |NULL       |Negative      |\n",
      "|Zelenskyy on Russian attack on Sumy: 'Only scum can act like this'                                                                                                                                                                                                                                        |worldnews    |NULL       |Negative      |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT title, subreddit, description, post_sentiment\n",
    "FROM posts\n",
    "WHERE score_3hr > 10000\n",
    "ORDER BY score_3hr DESC\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "# Example top post:\n",
    "# Zelenskyy on Russian attack on Sumy: 'Only scum can act like this'\n",
    "# Never interrupt your enemy while he is making a mistake.\n",
    "# Funny post: https://www.reddit.com/r/funny/comments/1jyc8gi/theres_always_one/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "l3aZNl_tO6lH"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;124mselect distinct category from posts;\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "\n",
    "select distinct category from posts;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hCb7knktysRs"
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "pNnhO7g5ysz_"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: streamlit in c:\\users\\owner\\anaconda3\\lib\\site-packages (1.37.1)\n",
      "Requirement already satisfied: pyspark in c:\\users\\owner\\anaconda3\\lib\\site-packages (3.5.5)\n",
      "Requirement already satisfied: pandas in c:\\users\\owner\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: altair<6,>=4.0 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from streamlit) (5.0.1)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from streamlit) (1.6.2)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from streamlit) (5.3.3)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from streamlit) (8.1.7)\n",
      "Requirement already satisfied: numpy<3,>=1.20 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from streamlit) (1.26.4)\n",
      "Requirement already satisfied: packaging<25,>=20 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from streamlit) (24.1)\n",
      "Requirement already satisfied: pillow<11,>=7.1.0 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from streamlit) (10.4.0)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from streamlit) (4.25.3)\n",
      "Requirement already satisfied: pyarrow>=7.0 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from streamlit) (16.1.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from streamlit) (2.32.3)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from streamlit) (13.7.1)\n",
      "Requirement already satisfied: tenacity<9,>=8.1.0 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from streamlit) (8.2.3)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3.0 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from streamlit) (4.11.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from streamlit) (3.1.43)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from streamlit) (0.8.0)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from streamlit) (6.4.1)\n",
      "Requirement already satisfied: watchdog<5,>=2.1.5 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from streamlit) (4.0.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from pyspark) (0.10.9.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
      "Requirement already satisfied: toolz in c:\\users\\owner\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (0.12.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\owner\\anaconda3\\lib\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.7)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2024.8.30)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from rich<14,>=10.14.0->streamlit) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from rich<14,>=10.14.0->streamlit) (2.15.1)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.10.6)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install streamlit pyspark pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting praw\n",
      "  Downloading praw-7.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting prawcore<3,>=2.4 (from praw)\n",
      "  Downloading prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting update_checker>=0.18 (from praw)\n",
      "  Downloading update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from praw) (1.8.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from prawcore<3,>=2.4->praw) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2024.8.30)\n",
      "Downloading praw-7.8.1-py3-none-any.whl (189 kB)\n",
      "Downloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
      "Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
      "Installing collected packages: update_checker, prawcore, praw\n",
      "Successfully installed praw-7.8.1 prawcore-2.4.0 update_checker-0.18.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install praw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud-pubsub\n",
      "  Downloading google_cloud_pubsub-2.29.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting grpcio<2.0.0,>=1.51.3 (from google-cloud-pubsub)\n",
      "  Downloading grpcio-1.71.0-cp312-cp312-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting google-auth<3.0.0,>=2.14.1 (from google-cloud-pubsub)\n",
      "  Downloading google_auth-2.39.0-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.0 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.0->google-cloud-pubsub)\n",
      "  Downloading google_api_core-2.24.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting proto-plus<2.0.0,>=1.22.0 (from google-cloud-pubsub)\n",
      "  Downloading proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from google-cloud-pubsub) (4.25.3)\n",
      "Collecting grpc-google-iam-v1<1.0.0,>=0.12.4 (from google-cloud-pubsub)\n",
      "  Downloading grpc_google_iam_v1-0.14.2-py3-none-any.whl.metadata (9.1 kB)\n",
      "Collecting grpcio-status>=1.33.2 (from google-cloud-pubsub)\n",
      "  Downloading grpcio_status-1.71.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting opentelemetry-api>=1.27.0 (from google-cloud-pubsub)\n",
      "  Downloading opentelemetry_api-1.32.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting opentelemetry-sdk>=1.27.0 (from google-cloud-pubsub)\n",
      "  Downloading opentelemetry_sdk-1.32.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.0->google-cloud-pubsub)\n",
      "  Downloading googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.0->google-cloud-pubsub) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-pubsub) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-pubsub) (0.2.8)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3.0.0,>=2.14.1->google-cloud-pubsub)\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 (from google-cloud-pubsub)\n",
      "  Downloading protobuf-5.29.4-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.27.0->google-cloud-pubsub)\n",
      "  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from opentelemetry-api>=1.27.0->google-cloud-pubsub) (7.0.1)\n",
      "Collecting opentelemetry-semantic-conventions==0.53b1 (from opentelemetry-sdk>=1.27.0->google-cloud-pubsub)\n",
      "  Downloading opentelemetry_semantic_conventions-0.53b1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from opentelemetry-sdk>=1.27.0->google-cloud-pubsub) (4.11.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from deprecated>=1.2.6->opentelemetry-api>=1.27.0->google-cloud-pubsub) (1.14.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.27.0->google-cloud-pubsub) (3.17.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-cloud-pubsub) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.0->google-cloud-pubsub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.0->google-cloud-pubsub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.0->google-cloud-pubsub) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.0->google-cloud-pubsub) (2024.8.30)\n",
      "Downloading google_cloud_pubsub-2.29.0-py2.py3-none-any.whl (317 kB)\n",
      "Downloading google_api_core-2.24.2-py3-none-any.whl (160 kB)\n",
      "Downloading google_auth-2.39.0-py2.py3-none-any.whl (212 kB)\n",
      "Downloading grpc_google_iam_v1-0.14.2-py3-none-any.whl (19 kB)\n",
      "Downloading grpcio-1.71.0-cp312-cp312-win_amd64.whl (4.3 MB)\n",
      "   ---------------------------------------- 0.0/4.3 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.3/4.3 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.5/4.3 MB 1.5 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 1.0/4.3 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 1.3/4.3 MB 1.8 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 1.8/4.3 MB 1.9 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.4/4.3 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 2.9/4.3 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 3.4/4.3 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 3.9/4.3 MB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.3/4.3 MB 2.2 MB/s eta 0:00:00\n",
      "Downloading grpcio_status-1.71.0-py3-none-any.whl (14 kB)\n",
      "Downloading opentelemetry_api-1.32.1-py3-none-any.whl (65 kB)\n",
      "Downloading opentelemetry_sdk-1.32.1-py3-none-any.whl (118 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.53b1-py3-none-any.whl (188 kB)\n",
      "Downloading proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading protobuf-5.29.4-cp310-abi3-win_amd64.whl (434 kB)\n",
      "Downloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
      "Downloading googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Installing collected packages: rsa, protobuf, grpcio, deprecated, proto-plus, opentelemetry-api, googleapis-common-protos, google-auth, opentelemetry-semantic-conventions, grpcio-status, google-api-core, opentelemetry-sdk, grpc-google-iam-v1, google-cloud-pubsub\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.25.3\n",
      "    Uninstalling protobuf-4.25.3:\n",
      "      Successfully uninstalled protobuf-4.25.3\n",
      "Successfully installed deprecated-1.2.18 google-api-core-2.24.2 google-auth-2.39.0 google-cloud-pubsub-2.29.0 googleapis-common-protos-1.70.0 grpc-google-iam-v1-0.14.2 grpcio-1.71.0 grpcio-status-1.71.0 opentelemetry-api-1.32.1 opentelemetry-sdk-1.32.1 opentelemetry-semantic-conventions-0.53b1 proto-plus-1.26.1 protobuf-5.29.4 rsa-4.9.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install google-cloud-pubsub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
